{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 13 - Clustering I: KMeans\n",
    "Name: Dempsey Wade \n",
    "\n",
    "Class: CSCI 349 - Intro to Data Mining \n",
    "\n",
    "Semester: 2019SP \n",
    "\n",
    "Instructor: Brian King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # for plot styling\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, homogeneity_completeness_v_measure, adjusted_rand_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [P] Copy the following code to build a very basic set of clustered data using the make_blobs function in sklearn.datasets.samples_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=60, centers=5,cluster_std=(0.3,0.4,0.5,0.7,0.7),center_box=(0, 8), random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [P] Convert your data into a single pandas data frame with three variables, \"x\", \"y\", and \"target\". The variable \"target\" will represent our ground truth, i.e. the correct cluster class. Be sure it is a true categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.920853</td>\n",
       "      <td>6.701244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>7.349579</td>\n",
       "      <td>7.531876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.415558</td>\n",
       "      <td>6.811551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.471362</td>\n",
       "      <td>4.780079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2.536789</td>\n",
       "      <td>3.920516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         x         y\n",
       "0       1  3.920853  6.701244\n",
       "1       4  7.349579  7.531876\n",
       "2       3  2.415558  6.811551\n",
       "3       0  1.471362  4.780079\n",
       "4       3  2.536789  3.920516"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X\n",
    "X = pd.DataFrame(X)\n",
    "X = X.rename(columns = {0:'x', 1:'y'})\n",
    "y_true = pd.DataFrame(y_true)\n",
    "y_true = y_true.rename(columns = {0:'target'})\n",
    "\n",
    "df = pd.concat([y_true, X], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].astype('category')\n",
    "df['"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [P] Show the result of info() on these data, which should have two numeric variables, named \"x\" and  \"y\", and one category, named \"target\". Also show a table of the \"target\" variable showing the distribution of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [P] Create a scatterplot of the data, using the target variable to color each cluster. You should have five colored clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorMap = {0:'red', 1:'blue', 2:'yellow', 3:'green', 4:'black'}\n",
    "colors = [colorMap[i] for i in df.target]\n",
    "plt.scatter(df.x, df.y, c = colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) [P] Read the reference for KMeans: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "Now, create a clustering using KMeans. Let's assume you have prior knowledge that there are indeed 5\n",
    "clusters, but you aren't aware of ground truth. Thus, you can only assess the SSE and silhouette coefficient for\n",
    "your clustering.\n",
    "Print the inertia_ (SSE) and the silhouette_score results for this clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, random_state = 1234).fit(X_new)\n",
    "print('SSE =', kmeans.inertia_)\n",
    "print('Silhouetter score =', silhouette_score(X_new, kmeans.fit_predict(df)))\n",
    "#print('Silhouetter score =', silhouette_score(df, kmeans.fit_predict(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) [P] Generate a scatterplot again, however, now color the points based off of the labels generated by the clustering. Add the centroids using a distinguishing color and size, such as black with an alpha=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = pd.DataFrame(kmeans.cluster_centers_)\n",
    "center = center.drop(0, axis = 1)\n",
    "center = center.rename(columns = {1:'x',2:'y'})\n",
    "\n",
    "##My 'kmeans.cluster_centers_' variable was giving \n",
    "#me a hard time so I converted it to a DataFrame\n",
    "\n",
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorMap = {0:'red', 1:'blue', 2:'yellow', 3:'green', 4:'black'}\n",
    "colors = [colorMap[i] for i in df.target]\n",
    "plt.scatter(df.x, df.y, c = kmeans.labels_)\n",
    "plt.scatter(center.x, center.y, marker = 'X', color = 'green') #Green is easier than black with alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [P] Let's take a step backward and assume that you aren't even sure about how many clusters you have. Evaluate cluster sizes from 2 through 10. However, create a data frame that stores the SSE and the silhouette score for each k. Show the resulting data frame, and show a line plot of both on separate graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({1:[1, 1]})\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters = i, random_state = 1234).fit(X_new)\n",
    "    sse = kmeans.inertia_\n",
    "    sil = silhouette_score(X_new, kmeans.fit_predict(X_new))\n",
    "    temp = pd.DataFrame({i:[sse, sil]})\n",
    "    results = results.join(temp)\n",
    "\n",
    "results = results.drop(results.columns[0], axis = 1)\n",
    "results = results.T\n",
    "results = results.rename(columns = {0:'SSE',1:'Silhouette'})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = results.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) [M] What does the SSE suggest is the best value of k? What about the silhouette coefficient? Why do you think there are these discrepancies? (If you have the interest, you should go back and regenerate your data with much smaller cluster_std values... and you'll see much more consistent results! That tells you something about how well these techniques work when your data are not linearly separable, and when your clusters are not all equally distributed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficiency suggests a k = 3. \n",
    "\n",
    "Because my SSE is so high for a value of k = 2, my graph is a little hard to read. From my DataFrame, as one could easily predict, the SSE continues to decrease as k gets larger. We want to find the point in the graph where the SSE levels off without there being to large of a SSE value. My dataFrame suggests a value of 5, since its SSE value is very low, even though my silhouette score off. \n",
    "\n",
    "For my silhouette score, k = 3 yields a 0.7, while k = 5 yields ~0.62. These values are very close, but the SSE for k = 3 and 5 are not close. Therefore my best clustering value would be anywhere from 3-5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) [P] Re-generate your clustering for k=5. You will likely notice that your labels will be different colors than your original labels. That's annoying, but there's not much you can do about it unless you reassign your labels using a simple mapping.\n",
    "Let's bring in our ground truth. Write the code to remap your cluster labels to the same labels as your ground\n",
    "truth.\n",
    "\n",
    "(HINT: How? Perhaps the easiest approach is to use the contingency_matrix function in\n",
    "sklearn.metrics.cluster package. Show the output of that, then think about how you can use that to\n",
    "get the best mapping.)\n",
    "\n",
    "One you have your cluster labels remapped, then plot your data, but keep every point that has been assigned\n",
    "the correct cluster label as black, and show those points that were not grouped into the correct cluster as red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, random_state = 0).fit(df)\n",
    "print('SSE =', kmeans.inertia_)\n",
    "print('Silhouetter score =', silhouette_score(df, kmeans.fit_predict(df)))\n",
    "\n",
    "center = pd.DataFrame(kmeans.cluster_centers_)\n",
    "center = center.drop(0, axis = 1)\n",
    "center = center.rename(columns = {1:'x',2:'y'})\n",
    "\n",
    "colorMap = {0:'red', 1:'blue', 2:'yellow', 3:'green', 4:'black'}\n",
    "colors = [colorMap[i] for i in df.target]\n",
    "plt.scatter(df.x, df.y, c = kmeans.labels_)\n",
    "plt.scatter(center.x, center.y, marker = 'X', color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) [M] Since we do have ground truth, there are several metrics that are used to assess the quality of your clustering. Read the section on clustering performance evaluation in Scikit-learn's documentation page:\n",
    "https://scikitlearn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "\n",
    "Pay close attention to the following: Adjusted Rand Index, Homogeneity, Completeness, and V-measure. In this section, very briefly describe each of these four measures. Though we did not explicitly cover them in class, these are good to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted Range Index- measures the similarity of two assignments.\n",
    "\n",
    "Homogeneity- an objective for clusters in which each cluster contains only members of a single class. \n",
    "\n",
    "Completeness- another objective for clusters in which all members of a class are assigned to the same cluster. You can have multiple instances of completeness in one graph. \n",
    "\n",
    "V-measure- The mean between homogeneity and completness. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) [P] Create a new data frame that contains these four measures for all KMeans clusterings of k between 2-10. As before, show your data frame, then generate a plot for each. However, since each of these falls on the same scale, generate them on either one plot, or show them as subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({1:[1, 1, 1, 1]})\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters = i, random_state = 0).fit(df)\n",
    "    ari = 1\n",
    "    homogeneity = 1\n",
    "    completeness = completeness_score(df.x, df.y)\n",
    "    v = 2*(homogeneity * completeness) / (homogeneity + completeness)\n",
    "    temp = pd.DataFrame({i:[ari, homogeneity, completeness, v]})\n",
    "    results = results.join(temp)\n",
    "    \n",
    "results = results.drop(results.columns[0], axis = 1)\n",
    "results = results.T\n",
    "results = results.rename(columns = {0:'ARI',1:'Homogeneity',2:'Completeness',3:'Vmeasure'})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(results.ARI, results.index)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(results.Homogeneity, results.index)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(results.Completeness, results.index)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(results.Vmeasure, results.index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) [P] OPTIONAL â€“ A benchmark dataset has been going around called the diamonds dataset. It's a rich dataset of 53940 rows over 10 variables. Each row represents various characteristics of diamonds. The primary motivation of these data is to challenge the machine learning community to predict the price of a diamond, based on the other 9 characteristics.\n",
    "\n",
    "Load in the data as follows:\n",
    "df_diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "This page has good info about the variables: https://www.kaggle.com/shivam2503/diamonds\n",
    "\n",
    "Your aim right now is to simply assess the following - are there any natural clusterings of these data? If so,\n",
    "over which variables? How many clusters?\n",
    "\n",
    "(NOTE: don't spend too much time on this yet. The next lab will repeat this question, but allow you to use\n",
    "Hierarchical Agglomerative clustering to help you answer the question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
